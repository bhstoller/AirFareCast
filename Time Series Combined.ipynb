{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9173cb-a84b-4c5e-8446-4008d83f5993",
   "metadata": {},
   "source": [
    "# EDA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1563a4-0cc0-47d6-b53b-6da733975c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63ce9dd-6bec-4fb8-b129-bc5a661489dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/casey/Documents/GitHub/AirFareCast/itineraries_snappy.parquet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3037d-449c-4ca7-a7be-53695afb9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40538bbd-d2d8-4fe8-927a-9126f334799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departure in datetime format\n",
    "df['flightDate'] = pd.to_datetime(df['flightDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946039ff-394b-4aef-8b34-6c7042878fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified on departure date\n",
    "df['strata'] = df['flightDate'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c86795-2632-45f0-bc9d-894e0fcdce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500,000 rows\n",
    "fraction = 500000 / len(df)\n",
    "\n",
    "sampled_df = df.groupby('strata', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=fraction, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06afd6f-c62c-4aab-a814-76eeaf68134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample size: {len(sampled_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c67814-4552-41b4-8058-d50aa3fa7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.read_csv(\"/Users/casey/Documents/GitHub/AirFareCast/itineraries_sample_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cbe001-d914-48f5-8e5b-77ff6f81e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_csv('itineraries_sample_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae5f3d-8459-4c5b-bd68-0f2e59cac033",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f2ecf-a639-4024-a296-dc9f4c7f0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "sampled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedeacd-88a9-4f8f-94de-04d784942128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(sampled_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fe5c1-0790-49a5-b975-e2bd23287a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(sampled_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de4db5-c274-4ddf-97cd-5f871036b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical variables\n",
    "sampled_df.hist(figsize=(12, 8), bins=30)\n",
    "plt.suptitle(\"Distribution of Numerical Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf90def-bf3a-43c8-a2a2-2f14e886a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Histogram of Total Fare Prices\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(sampled_df['totalFare'], bins=30, kde=True, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Total Fare Prices\", fontsize=16)\n",
    "plt.xlabel(\"Total Fare Price\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.ylim(0, 250000)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52024fc-0121-4255-9fbe-435c26ae1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line raph of Travel Duration\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x='flightDate', y='travelDuration', data=sampled_df, color='skyblue')\n",
    "plt.title(\"Travel Duration Over Time\", fontsize=16)\n",
    "plt.xlabel(\"Flight Date\", fontsize=14)\n",
    "plt.ylabel(\"Travel Duration (minutes)\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Improve the y-axis by formatting the travel duration\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels([f'{int(y_tick // 60)}h {int(y_tick % 60)}m' for y_tick in y_ticks])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386d21b-a0c2-41ef-bbbf-5230caf7c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart of Seat Availability Categories\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='seatsRemaining', data=sampled_df, palette='viridis')\n",
    "plt.title(\"Seat Availability Categories\", fontsize=16)\n",
    "plt.xlabel(\"Seat Availability\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09471a4f-af4f-4914-b161-342a80e9789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Line Plot of Fare Prices Over Time\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x='flightDate', y='totalFare', data=sampled_df, color='skyblue')\n",
    "plt.title(\"Total Fare Prices Over Time\", fontsize=16)\n",
    "plt.xlabel(\"Flight Date\", fontsize=14)\n",
    "plt.ylabel(\"Total Fare Price ($)\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Format the y-axis labels to include the dollar sign\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels([f'${int(y_tick)}' for y_tick in y_ticks])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d500e5-d817-4a37-9b71-f165e6b8ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'flightDate' is in datetime format\n",
    "sampled_df['flightDate'] = pd.to_datetime(sampled_df['flightDate'])\n",
    "\n",
    "# Create a new column 'dayOfWeek' in the dataframe\n",
    "sampled_df['dayOfWeek'] = sampled_df['flightDate'].dt.day_name()\n",
    "\n",
    "# Boxplot of Prices by Day of the Week\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='dayOfWeek', y='totalFare', data=sampled_df, palette='viridis')\n",
    "plt.title(\"Total Fare Prices by Day of the Week\", fontsize=16)\n",
    "plt.xlabel(\"Day of the Week\", fontsize=14)\n",
    "plt.ylabel(\"Total Fare Price ($)\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Format the y-axis labels to include the dollar sign\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels([f'${int(y_tick)}' for y_tick in y_ticks])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83c5c5-b9bb-4d80-be32-bc2f7e765d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart of Airline vs. Average Fare\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='segmentsAirlineName', y='totalFare', data=sampled_df, palette='viridis')\n",
    "plt.title(\"Average Fare by Airline\", fontsize=16)\n",
    "plt.xlabel(\"Airline\", fontsize=14)\n",
    "plt.ylabel(\"Average Fare Price ($)\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Format the y-axis labels to include the dollar sign\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels([f'${int(y_tick)}' for y_tick in y_ticks])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b44110-e6c3-44ec-be04-21b2d47662fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart of Cabin Class vs. Price\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='segmentsCabinCode', y='totalFare', data=sampled_df, palette='viridis')\n",
    "plt.title(\"Average Fare by Cabin Class\", fontsize=16)\n",
    "plt.xlabel(\"Cabin Class\", fontsize=14)\n",
    "plt.ylabel(\"Average Fare Price ($)\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Format the y-axis labels to include the dollar sign\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels([f'${int(y_tick)}' for y_tick in y_ticks])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f4bcc-44d5-443f-b918-b81be2cebfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517cd262-541b-4f68-9c76-8cf44e0348d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Feature Correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "numeric_cols = sampled_df.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlations Heatmap\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f2f7e-c9d8-4f21-b094-35f6fd928382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot of Distance vs. Price\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='totalTravelDistance', y='totalFare', data=sampled_df, color='skyblue')\n",
    "plt.title(\"Distance vs. Total Fare Price\", fontsize=16)\n",
    "plt.xlabel(\"Distance (miles)\", fontsize=14)\n",
    "plt.ylabel(\"Total Fare Price ($)\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Format the y-axis labels to include the dollar sign\n",
    "y_ticks = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels([f'${int(y_tick)}' for y_tick in y_ticks])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d7f3f-cd03-4d36-af05-be4ed80c76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Distribution by Time of Day\n",
    "df['departure_hour'] = pd.to_datetime(df['segmentsDepartureTimeRaw'].str.split('||').str[0]).dt.hour\n",
    "plt.subplot(3, 2, 1)\n",
    "avg_price_by_hour = df.groupby('departure_hour')['totalFare'].mean().dropna()\n",
    "sns.lineplot(x=avg_price_by_hour.index, y=avg_price_by_hour.values)\n",
    "plt.title('Average Fare by Departure Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Fare ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb91eb-ab9f-4886-8c21-bd809ae220c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Distribution by Day of Week\n",
    "df['departure_day'] = df['segmentsDepartureTimeRaw'].str.split('||').apply(lambda x: pd.to_datetime(x[0]).day_name())\n",
    "plt.subplot(3, 2, 2)\n",
    "avg_price_by_day = df.groupby('departure_day')['totalFare'].mean()\n",
    "sns.barplot(x=avg_price_by_day.index, y=avg_price_by_day.values)\n",
    "plt.title('Average Fare by Day of Week')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Fare ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf35889-0648-44b3-b6cd-4ae1823602a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Distribution by Month\n",
    "df['departure_month'] = df['segmentsDepartureTimeRaw'].str.split('||').apply(lambda x: pd.to_datetime(x[0]).month_name())\n",
    "plt.subplot(3, 2, 3)\n",
    "avg_price_by_month = df.groupby('departure_month')['totalFare'].mean()\n",
    "sns.barplot(x=avg_price_by_month.index, y=avg_price_by_month.values)\n",
    "plt.title('Average Fare by Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Fare ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b73be-78db-4383-9d8a-73336d507cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price vs Seats Remaining\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.boxplot(x='seatsRemaining', y='totalFare', data=df)\n",
    "plt.title('Fare Distribution by Seats Remaining')\n",
    "plt.ylabel('Total Fare ($)')\n",
    "\n",
    "# Non-Stop vs Connecting Flight Prices\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.boxplot(x='isNonStop', y='totalFare', data=df)\n",
    "plt.title('Fare Distribution: Non-Stop vs Connecting Flights')\n",
    "plt.ylabel('Total Fare ($)')\n",
    "\n",
    "# Basic Economy vs Regular Economy Prices\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.boxplot(x='isBasicEconomy', y='totalFare', data=df)\n",
    "plt.title('Fare Distribution: Basic vs Regular Economy')\n",
    "plt.ylabel('Total Fare ($)')\n",
    "\n",
    "# Price vs Distance\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.scatterplot(x='totalTravelDistance', y='totalFare', data=df, alpha=0.1)\n",
    "plt.title('Fare vs Travel Distance')\n",
    "plt.xlabel('Travel Distance')\n",
    "plt.ylabel('Total Fare ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b070e08-22bf-4334-8bad-a2aebc87e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detailed Price Analysis:\")\n",
    "\n",
    "# Price statistics by flight type\n",
    "nonstop_stats = df[df['isNonStop']]['totalFare'].describe()\n",
    "connecting_stats = df[~df['isNonStop']]['totalFare'].describe()\n",
    "\n",
    "print(\"\\nNon-Stop Flight Prices:\")\n",
    "print(nonstop_stats)\n",
    "print(\"\\nConnecting Flight Prices:\")\n",
    "print(connecting_stats)\n",
    "\n",
    "# Price variation by number of seats remaining\n",
    "print(\"\\nAverage Prices by Seats Remaining:\")\n",
    "print(df.groupby('seatsRemaining')['totalFare'].mean().round(2))\n",
    "\n",
    "# Price percentiles\n",
    "percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "price_percentiles = df['totalFare'].quantile(percentiles)\n",
    "print(\"\\nPrice Percentiles:\")\n",
    "for p, value in zip(percentiles, price_percentiles):\n",
    "    print(f\"{int(p*100)}th percentile: ${value:.2f}\")\n",
    "\n",
    "# Price variability metrics\n",
    "print(\"\\nPrice Variability Metrics:\")\n",
    "print(f\"Standard Deviation: ${df['totalFare'].std():.2f}\")\n",
    "print(f\"Coefficient of Variation: {(df['totalFare'].std() / df['totalFare'].mean() * 100):.1f}%\")\n",
    "\n",
    "# Markup analysis\n",
    "df['markup_percentage'] = ((df['totalFare'] - df['baseFare']) / df['baseFare'] * 100)\n",
    "print(\"\\nMarkup Analysis:\")\n",
    "print(f\"Average Markup: {df['markup_percentage'].mean():.1f}%\")\n",
    "print(f\"Median Markup: {df['markup_percentage'].median():.1f}%\")\n",
    "print(f\"Maximum Markup: {df['markup_percentage'].max():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299635e-8b1c-42c9-8529-594c8c8294ac",
   "metadata": {},
   "source": [
    "# Models DF, RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b003d-9149-4acd-9270-4bbaec9aa2ed",
   "metadata": {},
   "source": [
    "# Import Library\n",
    "\n",
    "`data_loading` and `feature_engineering` are python files that I wrote. `data_loading` contains a function called load_data which you will use to load the dataframe, and `feature_engineering` contains a function called apply_feature_engineering which you will use to apply the feature engineering (so that we all use the same processed data in ML models).\n",
    "\n",
    "**Before running this script, make sure you have downloaded 'itineraries_snappy.parquet' and are storing in a folder called 'data'**\n",
    "\n",
    "You can upload these as normal libaries, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449789c-b0ee-4ce9-9a7c-52e539171c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering import apply_feature_engineering, add_dummies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a196a0-1757-476e-9b0d-fd027a38541c",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Here is where you will call the load_data function from data_loading --> there are no parameters needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1a792-b9a7-4751-b9bc-4a9ad02c16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the dataset using parquet and pyarrow\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(\n",
    "        \"itineraries_snappy.parquet\", \n",
    "        engine= \"pyarrow\", \n",
    "        columns= [\n",
    "            \"searchDate\", \n",
    "            \"flightDate\", \n",
    "            \"startingAirport\", \n",
    "            \"destinationAirport\",\n",
    "            \"travelDuration\", \n",
    "            \"isBasicEconomy\", \n",
    "            \"isRefundable\", \n",
    "            \"isNonStop\", \n",
    "            \"totalFare\", \n",
    "            \"seatsRemaining\", \n",
    "            \"totalTravelDistance\",\n",
    "            \"segmentsDepartureTimeRaw\", \n",
    "            \"segmentsAirlineCode\", \n",
    "            \"segmentsCabinCode\"\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc6dfd-91a7-4259-9be1-77743284b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load_data to get the data as a pandas dataframe\n",
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347c8fd-2c11-4f06-a751-2d9f5a0b2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is too large to use in entirety, set a sample of 800,000 rows\n",
    "sample_size = 800000\n",
    "\n",
    "# Get the first 800,000 rows\n",
    "df_sample = df.iloc[:sample_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead9731-454d-45ba-a40d-ff4a3eab6ed0",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Here is where you will call the apply_feature_engineering function from feature_engineering --> there are no parameters needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadfddca-8edb-41f5-b972-19a806816ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the apply_feature_engineering function from feature_engineering to get the data ready for ML Modeling\n",
    "df_sample = apply_feature_engineering(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75027ab-57d8-4b9a-8ece-b2d22c80b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should see the following columns and data types\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc36ba5-dc8f-4079-86cb-4846a6295249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 5 rows should look like this\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78adb34-89a9-4c6f-83b0-9d9e14c05293",
   "metadata": {},
   "source": [
    "# Example ML Modeling: Decision Tree\n",
    "You can now use sklearn as normal --> see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c52af-04df-4aba-81d4-a690c210de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate decision tree regressor (since we predicting price, not classifying)\n",
    "dt = DecisionTreeRegressor(random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae75b7d-68da-4b1e-b509-e74826086d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our X variables in these models will be all columns that are not price\n",
    "X = df_sample.drop(columns= ['totalFare'], axis= 1)\n",
    "\n",
    "# Our y variable is of course price which is called 'totalFare'\n",
    "y = df_sample['totalFare']\n",
    "\n",
    "# Split the data into train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd1520-27d2-41f1-aac4-e1dd8be796cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict the data\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1aeb5-3d65-4912-b331-f7d9e9f68ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the error metrics using four decimal places\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Sqaured Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error {rmse:0.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape:.4%}\")\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1542a02-0b54-44a9-b624-156ff2faa78e",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a9c9-924e-4d42-ae41-ca1798604ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df_sample.drop(columns=['totalFare'], axis=1)\n",
    "y = df_sample['totalFare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7eb07b-0808-446d-af6c-3cdd4cec047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling category columns\n",
    "for col in X.select_dtypes(include=['category']).columns:\n",
    "    X[col] = X[col].cat.codes\n",
    "\n",
    "# Train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29752232-ba2d-42e1-bc98-892fa86e0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance metrics\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance Metrics:\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape:.4%}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return y_pred, mae, rmse, mape, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d02d8-c762-4acc-9d40-d6aadecc2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Smaller sample for hyperparameter tuning to save time\n",
    "sample_indices = np.random.choice(len(X_train), min(100000, len(X_train)), replace=False)\n",
    "X_train_sample = X_train.iloc[sample_indices]\n",
    "y_train_sample = y_train.iloc[sample_indices]\n",
    "print(f\"Using {len(X_train_sample)} samples for hyperparameter tuning\")\n",
    "\n",
    "# Decision Tree Hyperparameter Tuning\n",
    "print(\"\\n--- Decision Tree Hyperparameter Tuning ---\")\n",
    "dt_param_grid = {\n",
    "    'max_depth': [10, 15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=dt_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Decision Tree models...\")\n",
    "start_time = time.time()\n",
    "dt_grid.fit(X_train_sample, y_train_sample)\n",
    "dt_tuning_time = time.time() - start_time\n",
    "print(f\"Decision Tree tuning completed in {dt_tuning_time:.2f} seconds\")\n",
    "\n",
    "print(\"Best Decision Tree parameters:\")\n",
    "print(dt_grid.best_params_)\n",
    "print(f\"Best CV score: {-dt_grid.best_score_:.4f} RMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2c13b-16e2-4ac4-b706-a5e5865299a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best Decision Tree model\n",
    "best_dt = dt_grid.best_estimator_\n",
    "best_dt.fit(X_train, y_train)  # Refit on full training data\n",
    "dt_val_pred, dt_val_mae, dt_val_rmse, dt_val_mape, dt_val_r2 = evaluate_model(\n",
    "    best_dt, X_test, y_test, \"Best Decision Tree (Validation)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c850c-2fd9-48b7-afa7-b5294558f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model with the best parameters on the whole dataset\n",
    "dt_best = DecisionTreeRegressor(random_state=42, **dt_grid.best_params_)\n",
    "dt_best.fit(X_train, y_train)\n",
    "dt_best_pred, dt_best_mae, dt_best_rmse, dt_best_mape, dt_best_r2 = evaluate_model(dt_best, X_test, y_test, \"Best Decision Tree\")\n",
    "\n",
    "#Saving the model results \n",
    "model_results = {\n",
    "    \"Model\": [\"Decision Tree\", \"Best Decision Tree\", \"Best Decision Tree (Validation)\"],\n",
    "    \"MAE\": [mae, dt_best_mae, dt_val_mae],\n",
    "    \"RMSE\": [rmse, dt_best_rmse, dt_val_rmse],\n",
    "    \"MAPE\": [mape, dt_best_mape, dt_val_mape],\n",
    "    \"R²\": [r2, dt_best_r2, dt_val_r2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560edc7c-ea8c-492e-8442-3c381dcee412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals\n",
    "residuals = y_test - dt_best_pred\n",
    "\n",
    "#Plotting the residuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='skyblue')\n",
    "plt.title(\"Residuals Distribution for the Best Decision Tree Model\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba036a-edb9-4759-93bf-12e400f12dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, dt_best_pred, alpha=0.3, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Total Fare')\n",
    "plt.ylabel('Predicted Total Fare')\n",
    "plt.title('Predicted vs Actual Total Fare for the Best Decision Tree Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb732bb-3a55-4092-8b1b-c7e7085d08c8",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fe037-cd67-46db-9f6c-dfbc961851d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Random Forest Hyperparameter Tuning\n",
    "print(\"\\n--- Random Forest Hyperparameter Tuning ---\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest models...\")\n",
    "start_time = time.time()\n",
    "rf_grid.fit(X_train_sample, y_train_sample)\n",
    "rf_tuning_time = time.time() - start_time\n",
    "print(f\"Random Forest tuning completed in {rf_tuning_time:.2f} seconds\")\n",
    "\n",
    "print(\"Best Random Forest parameters:\")\n",
    "print(rf_grid.best_params_)\n",
    "print(f\"Best CV score: {-rf_grid.best_score_:.4f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fd52a-8323-4a04-90ef-3a3305c53ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best Random Forest model\n",
    "best_rf = rf_grid.best_estimator_\n",
    "best_rf.fit(X_train, y_train)  \n",
    "rf_val_pred, rf_val_mae, rf_val_rmse, rf_val_mape, rf_val_r2 = evaluate_model(\n",
    "    best_rf, X_test, y_test, \"Best Random Forest (Validation)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53433e4-88d4-49bb-907f-5979599487da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model with the best parameters on the whole dataset\n",
    "rf_best = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_grid.best_params_)\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_best_pred, rf_best_mae, rf_best_rmse, rf_best_mape, rf_best_r2 = evaluate_model(rf_best, X_test, y_test, \"Best Random Forest\")\n",
    "\n",
    "#Saving the model results\n",
    "model_results[\"Model\"].extend([\"Random Forest\", \"Best Random Forest\", \"Best Random Forest (Validation)\"])\n",
    "model_results[\"MAE\"].extend([mae, rf_best_mae, rf_val_mae])\n",
    "model_results[\"RMSE\"].extend([rmse, rf_best_rmse, rf_val_rmse])\n",
    "model_results[\"MAPE\"].extend([mape, rf_best_mape, rf_val_mape])\n",
    "model_results[\"R²\"].extend([r2, rf_best_r2, rf_val_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61142c-0542-4fef-b2d8-cea8966b3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals\n",
    "residuals_rf = y_test - rf_best_pred\n",
    "\n",
    "#Plotting the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals_rf, kde=True, color='skyblue')\n",
    "plt.title(\"Residuals Distribution for the Best Random Forest Model\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6214c4-cdd8-4cf0-aac2-c6d5ce0b81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, rf_best_pred, alpha=0.3, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Total Fare')\n",
    "plt.ylabel('Predicted Total Fare')\n",
    "plt.title('Predicted vs Actual Total Fare for the Best Random Forest Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49a959-e210-4ea7-bf00-a57bead194a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Plot to check the distribution of errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rf_best_pred, residuals_rf, alpha=0.3, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(\"Residual Plot for the Best Random Forest Model\")\n",
    "plt.xlabel(\"Predicted Total Fare\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132df43-8a2c-4e39-b682-b134c12db6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance Visualization\n",
    "def plot_feature_importance(importance, names, model_type):\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    plt.title(model_type + ' Feature Importance')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature Names')\n",
    "\n",
    "plot_feature_importance(best_rf.feature_importances_, X_train.columns, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae924f-5427-4e4b-b69a-41ce2486382c",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae1975-ddbf-4ca3-9a44-8785b517e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Hyperparameter Tuning\n",
    "%pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(\"\\n--- XGBoost Hyperparameter Tuning ---\")\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost models...\")\n",
    "start_time = time.time()\n",
    "xgb_grid.fit(X_train_sample, y_train_sample)\n",
    "xgb_tuning_time = time.time() - start_time\n",
    "print(f\"XGBoost tuning completed in {xgb_tuning_time:.2f} seconds\")\n",
    "\n",
    "print(\"Best XGBoost parameters:\")\n",
    "print(xgb_grid.best_params_)\n",
    "print(f\"Best CV score: {-xgb_grid.best_score_:.4f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dca4ed-340e-47b0-9e15-7559e725bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best XGBoost model\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "best_xgb.fit(X_train, y_train)  \n",
    "xgb_val_pred, xgb_val_mae, xgb_val_rmse, xgb_val_mape, xgb_val_r2 = evaluate_model(\n",
    "    best_xgb, X_test, y_test, \"Best XGBoost (Validation)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0cc35-44a7-4dfd-957a-6dcad6acd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model with the best parameters on the whole dataset\n",
    "xgb_best = XGBRegressor(random_state=42, n_jobs=-1, **xgb_grid.best_params_)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "xgb_best_pred, xgb_best_mae, xgb_best_rmse, xgb_best_mape, xgb_best_r2 = evaluate_model(xgb_best, X_test, y_test, \"Best XGBoost\")\n",
    "\n",
    "#Saving the model results\n",
    "model_results[\"Model\"].extend([\"XGBoost\", \"Best XGBoost\", \"Best XGBoost (Validation)\"])\n",
    "model_results[\"MAE\"].extend([mae, xgb_best_mae, xgb_val_mae])\n",
    "model_results[\"RMSE\"].extend([rmse, xgb_best_rmse, xgb_val_rmse])\n",
    "model_results[\"MAPE\"].extend([mape, xgb_best_mape, xgb_val_mape])\n",
    "model_results[\"R²\"].extend([r2, xgb_best_r2, xgb_val_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a4e79-87a4-4b79-b5ec-17593d36f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals\n",
    "residuals_xgb = y_test - xgb_best_pred\n",
    "\n",
    "#Plotting the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals_xgb, kde=True, color='skyblue')\n",
    "plt.title(\"Residuals Distribution for the Best XGBoost Model\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c6607-cee2-4584-9dbf-fe97ef6e273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, xgb_best_pred, alpha=0.3, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Total Fare')\n",
    "plt.ylabel('Predicted Total Fare')\n",
    "plt.title('Predicted vs Actual Total Fare for the Best XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02450ead-ba99-4465-a0dd-1b2fc17b82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Plot to check the distribution of errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(xgb_best_pred, residuals_xgb, alpha=0.3, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(\"Residual Plot for the Best XGBoost Model\")\n",
    "plt.xlabel(\"Predicted Total Fare\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdde55-b9f0-45ca-a741-943f5fb899bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance Visualization\n",
    "plot_feature_importance(best_xgb.feature_importances_, X_train.columns, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4eaaf-69d6-40fd-a98e-03a2f51c59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table for all the model validations\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "model_results_df.set_index('Model', inplace=True)\n",
    "model_results_df\n",
    "\n",
    "# Plotting each metric separately\n",
    "metrics = ['MAE', 'RMSE', 'MAPE', 'R²']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    model_results_df[metric].plot(kind='bar', color='skyblue')\n",
    "    plt.title(f\"Model Comparison - {metric}\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc4946-3574-4e97-9e78-d1201add2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table for all the model validations\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "model_results_df.set_index('Model', inplace=True)\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a84da6-4b57-4bad-9d37-7ac273773898",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad679c9-44aa-4d1c-b3ad-ad9aaa4b06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50f516-9b2e-48a2-b707-f3dab9773323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_data()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26b51a-069c-45d9-8f46-39acdb77f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is too large to use in entirety, set a sample of 800,000 rows\n",
    "sample_size = 800000\n",
    "\n",
    "# Get the first 800,000 rows\n",
    "df_sample1 = df1.iloc[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594de944-912e-44b5-a562-39038f6c1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the apply_feature_engineering function from feature_engineering to get the data ready for ML Modeling\n",
    "df_sample1 = apply_feature_engineering(df_sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652727c4-2c91-4965-91c3-9213e6e6abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = df_sample1['totalFare'].values\n",
    "# Extracting the features\n",
    "X = df_sample1.drop(columns=['totalFare']).values\n",
    "\n",
    "# Training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d2407-202e-4d30-a303-f9cabdd3fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Scaling target\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86824dd5-448d-4000-8862-45c4eacd9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence for LSTM\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:(i + time_steps)])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "time_steps = 10 # How many steps to look back\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "\n",
    "# Shapes to verify\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}\")\n",
    "print(f\"y_test_seq shape: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824d7e8-18c8-4995-ad2b-eb5d253de422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    # LSTM layer with return sequences True for stacked LSTM\n",
    "    LSTM(50, activation='relu', return_sequences=True, \n",
    "         input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    Dropout(0.2),  # To prevent overfitting\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Output layer for regression\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Adam optimizer and mse loss\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d6e99-8e7c-4988-a799-b165ed9a9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with early stopping and model checkpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Model checkpoint to save the best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_lstm_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train \n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cb5c0-2640-480b-8bdc-2db3f7a9ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values by imputing with the mean of the respective columns\n",
    "X_train_seq = np.nan_to_num(X_train_seq, nan=np.nanmean(X_train_seq))\n",
    "X_test_seq = np.nan_to_num(X_test_seq, nan=np.nanmean(X_test_seq))\n",
    "y_train_seq = np.nan_to_num(y_train_seq, nan=np.nanmean(y_train_seq))\n",
    "y_test_seq = np.nan_to_num(y_test_seq, nan=np.nanmean(y_test_seq))\n",
    "\n",
    "# Model evaluation\n",
    "# Predictions\n",
    "y_train_pred_scaled = model.predict(X_train_seq)\n",
    "y_test_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse transform to the original scale\n",
    "y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)\n",
    "y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)\n",
    "y_train_actual = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test_actual, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "r2 = r2_score(y_test_actual, y_test_pred)\n",
    "mape = np.mean(np.abs((y_test_actual - y_test_pred) / y_test_actual)) * 100\n",
    "\n",
    "print(f'Training MSE: {mean_squared_error(y_train_actual, y_train_pred):.2f}')\n",
    "print(f'Test MSE: {mse:.2f}')\n",
    "print(f'Test RMSE: {rmse:.2f}')\n",
    "print(f'Test MAE: {mae:.2f}')\n",
    "print(f'Test R² Score: {r2:.4f}')\n",
    "print(f'Test MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d0321-3348-43dd-80a9-971d03e7d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATIONS\n",
    "\n",
    "# Training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Progression')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test_actual, y_test_pred, alpha=0.5, color='blue')\n",
    "plt.plot([y_test_actual.min(), y_test_actual.max()], \n",
    "         [y_test_actual.min(), y_test_actual.max()], \n",
    "         'r--', lw=2)\n",
    "plt.title('Actual vs Predicted Total Fare Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d8ffd-acc0-4070-b9e6-2436d313e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actual over time\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Check NaN values in the data\n",
    "y_test_actual_plot = np.nan_to_num(y_test_actual, nan=np.nanmean(y_test_actual))\n",
    "y_test_pred_plot = np.nan_to_num(y_test_pred, nan=np.nanmean(y_test_pred))\n",
    "\n",
    "plt.plot(y_test_actual_plot, label='Actual', color='blue', alpha=0.7)\n",
    "plt.plot(y_test_pred_plot, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Total Fare Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Total Fare')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test_actual_plot - y_test_pred_plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test_pred_plot, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residual Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd782e40-32a2-4107-bc88-b6ba825e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "# Since LSTM doesn't directly provide feature importance, we'll analyze correlations\n",
    "\n",
    "# Extracting feature names assuming df is available\n",
    "feature_names = df_sample1.drop(columns=['totalFare']).columns.tolist()\n",
    "\n",
    "# Function to calculate feature importance using permutation importance\n",
    "def permutation_importance(model, X, y, n_repeats=5):\n",
    "    \"\"\"Calculate permutation importance for features\"\"\"\n",
    "    baseline_loss = model.evaluate(X, y, verbose=0)\n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X.shape[2]):  # Loop through features\n",
    "        # Clone the dataset\n",
    "        X_permuted = X.copy()\n",
    "        # Calculate importance over multiple repeats\n",
    "        feature_importance = []\n",
    "        for _ in range(n_repeats):\n",
    "            # Permute the feature\n",
    "            for seq_idx in range(X.shape[0]):\n",
    "                np.random.shuffle(X_permuted[seq_idx, :, i])\n",
    "            # Calculate loss with permuted feature\n",
    "            permuted_loss = model.evaluate(X_permuted, y, verbose=0)\n",
    "            # Importance is the increase in loss\n",
    "            importance = permuted_loss - baseline_loss\n",
    "            feature_importance.append(importance)\n",
    "        # Take the mean importance across repeats\n",
    "        importances.append(np.mean(feature_importance))\n",
    "    \n",
    "    return importances\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = permutation_importance(model, X_test_seq, y_test_seq)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=feature_importance, y=feature_names, palette='viridis')\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d8cb9-bcfd-47bd-b1f2-723e5f6cd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: correlation with target\n",
    "correlations = [np.corrcoef(X_train[:, i], y_train.flatten())[0, 1] for i in range(X_train.shape[1])]\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "sorted_idx = np.argsort(np.abs(correlations))\n",
    "plt.barh(range(len(sorted_idx)), [abs(correlations[i]) for i in sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.title('Feature Importance (Correlation with Target)')\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4a2a3-429b-433e-8032-83d9cb064193",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21548eb0-07a3-495a-b43d-0e956613d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "\n",
    "# Data preparation\n",
    "df2= load_data()\n",
    "\n",
    "# The data is too large to use in entirety, set a sample of 800,000 rows\n",
    "sample_size = 800000\n",
    "\n",
    "# Get the first 800,000 rows\n",
    "df_sample2 = df2.iloc[:sample_size]\n",
    "\n",
    "# Call the apply_feature_engineering function from feature_engineering to get the data ready for ML Modeling\n",
    "df_sample2 = apply_feature_engineering(df_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091fb7db-e32b-49c4-bbc1-eee9b85f671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify and convert any datetime columns\n",
    "print(\"DataFrame info:\")\n",
    "print(df_sample2.info())\n",
    "print(\"\\nSample data:\")\n",
    "print(df_sample2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17054bec-6c91-4e73-b2b4-3c78f24f5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime columns\n",
    "datetime_columns = df_sample2.select_dtypes(include=['object']).columns\n",
    "for col in datetime_columns:\n",
    "    try:\n",
    "        # Try to convert to datetime\n",
    "        df_sample2[col] = pd.to_datetime(df_sample2[col])\n",
    "        print(f\"Converted {col} to datetime\")\n",
    "        \n",
    "        # Extract useful features from datetime\n",
    "        df_sample2[f\"{col}_year\"] = df_sample2[col].dt.year\n",
    "        df_sample2[f\"{col}_month\"] = df_sample2[col].dt.month\n",
    "        df_sample2[f\"{col}_day\"] = df_sample2[col].dt.day\n",
    "        df_sample2[f\"{col}_dayofweek\"] = df_sample2[col].dt.dayofweek\n",
    "        df_sample2[f\"{col}_hour\"] = df_sample2[col].dt.hour if hasattr(df[col].dt, 'hour') else 0\n",
    "        \n",
    "        # Drop the original datetime column\n",
    "        df_sample2 = df_sample2.drop(columns=[col])\n",
    "        print(f\"Created time features from {col} and dropped original column\")\n",
    "    except:\n",
    "        print(f\"Column {col} couldn't be converted to datetime, keeping as is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d4cdf-accb-4c43-9e96-bf8969ca81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any remaining object columns that might cause issues\n",
    "remaining_object_cols = df_sample2.select_dtypes(include=['object']).columns\n",
    "if len(remaining_object_cols) > 0:\n",
    "    print(f\"Warning: These columns are still object type and may cause issues: {list(remaining_object_cols)}\")\n",
    "    \n",
    "    # For remaining object columns, we'll use one-hot encoding\n",
    "    print(\"Applying one-hot encoding to categorical columns...\")\n",
    "    df_sample2 = pd.get_dummies(df_sample2, columns=remaining_object_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5387c-0ce7-4d34-8d34-590018f9ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target variable\n",
    "y = df_sample2['totalFare'].values\n",
    "# Extract features \n",
    "X = df_sample2.drop(columns=['totalFare']).values\n",
    "\n",
    "# Check for NaN values before splitting\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"Warning: NaN values detected in the data\")\n",
    "    print(f\"Number of NaN values in X: {np.isnan(X).sum()}\")\n",
    "    print(f\"Number of NaN values in y: {np.isnan(y).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d3854-5387-423b-a3c2-d576c1b0ce1f",
   "metadata": {},
   "source": [
    "# RNN & LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66405d10-83eb-4990-8c65-080a2c684cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering import apply_feature_engineering, add_dummies\n",
    "from data_loading import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3cb6d-734b-435b-8bde-0ad129491ba7",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e05972-2362-451e-9966-6f6ae9aeb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load_data to get the data as a pandas dataframe\n",
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d933f7-9377-435c-b8d6-6a7dc808bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is too large to use in entirety, set a sample of 800,000 rows\n",
    "sample_size = 800000\n",
    "\n",
    "# Get the first 800,000 rows\n",
    "df_sample = df.iloc[:sample_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537fba8-55a9-4afd-8e71-652be30def00",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9bf99-d479-498f-bd6f-7b8440ffd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the apply_feature_engineering function from feature_engineering to get the data ready for ML Modeling\n",
    "df_sample = apply_feature_engineering(df_sample, rnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3fac1-7860-4686-aa0c-76af481002b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f554d-f361-4e9c-81d5-7bc292fe53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample.to_csv('data/datasample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc0437-a5f3-4798-abaa-31a20bd97a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff5ee7-96fb-4b9f-8104-41d34e4366df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grouped = df_sample.groupby(['flightDate', 'startingAirport', 'destinationAirport'])\n",
    "\n",
    "airline_variation = grouped['airlineCode'].nunique().reset_index(name='unique_airlines')\n",
    "\n",
    "summary_stats = airline_variation['unique_airlines'].describe()\n",
    "print(\"Summary statistics for unique airlines per group:\")\n",
    "print(summary_stats)\n",
    "\n",
    "multi_airline_percentage = (airline_variation['unique_airlines'] > 1).mean() * 100\n",
    "print(f\"{multi_airline_percentage:.2f}% of groups have more than one airline.\")\n",
    "\n",
    "plt.hist(airline_variation['unique_airlines'], bins=range(1, airline_variation['unique_airlines'].max()+2), align='left')\n",
    "plt.xlabel(\"Unique Airlines per Group\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Unique Airlines in Each Group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61315245-f262-4594-b905-ef0b5d07c0c6",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43addec5-4a73-42c0-8bad-32ae045f3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c59e69-7c12-49b3-934f-e53e7631913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_by_group(df, sequence_length):\n",
    "    X_seq_list = []\n",
    "    y_seq_list = []\n",
    "\n",
    "    groups = df.groupby(['flightDate', 'startingAirport', 'destinationAirport'])\n",
    "\n",
    "    for group_keys, group_df in groups:\n",
    "        # Sort the group by daysToDeparture\n",
    "        group_df = group_df.sort_values(\"daysToDeparture\")\n",
    "\n",
    "        # Only build a sequence if the group is large enough.\n",
    "        if len(group_df) <= sequence_length:\n",
    "            continue\n",
    "\n",
    "        # Drop columns that are used for grouping (keep flight date for reference)\n",
    "        X_group = group_df.drop(columns=['totalFare', 'flightDate', 'startingAirport', 'destinationAirport'])\n",
    "\n",
    "        # Dropping Bugged columns for now:\n",
    "        X_group = X_group.drop(columns=['airlineCode', 'cabinClass'])\n",
    "\n",
    "        y_group = group_df['totalFare']\n",
    "\n",
    "        # Ensure the feature data is numeric (convert booleans to float, etc.)\n",
    "        X_group = X_group.astype('float32')\n",
    "\n",
    "        X_values = X_group.to_numpy()\n",
    "        y_values = y_group.to_numpy().reshape(-1, 1)\n",
    "\n",
    "        for i in range(len(X_values) - sequence_length):\n",
    "            X_seq_list.append(X_values[i:i+sequence_length])\n",
    "            y_seq_list.append(y_values[i + sequence_length])\n",
    "\n",
    "    return np.array(X_seq_list), np.array(y_seq_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d0b53-c1ed-467a-8b05-57a33ead034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60\n",
    "df_sample = df_sample.fillna(0)\n",
    "X_sequences, y_sequences = create_sequences_by_group(df_sample, sequence_length)\n",
    "\n",
    "print(\"X_sequences shape:\", X_sequences.shape)  # (num_sequences, sequence_length, num_features)\n",
    "print(\"y_sequences shape:\", y_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3965c1-760c-43f6-bc50-f9eb12dc2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "num_features = X_train_seq.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddee10a-b23f-497e-89cc-9d738af67fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler()\n",
    "X_train_seq_flat = X_train_seq.reshape(-1, num_features)\n",
    "scaler_X.fit(X_train_seq_flat)\n",
    "X_train_seq_scaled = scaler_X.transform(X_train_seq_flat).reshape(X_train_seq.shape)\n",
    "\n",
    "X_test_seq_flat = X_test_seq.reshape(-1, num_features)\n",
    "X_test_seq_scaled = scaler_X.transform(X_test_seq_flat).reshape(X_test_seq.shape)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(y_train_seq)\n",
    "y_train_seq_scaled = scaler_y.transform(y_train_seq)\n",
    "y_test_seq_scaled = scaler_y.transform(y_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995efee-6a8b-4cb6-8343-79f22bc2f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs in X_train_seq_scaled:\", np.isnan(X_train_seq_scaled).sum())\n",
    "print(\"NaNs in y_train_seq_scaled:\", np.isnan(y_train_seq_scaled).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c159c00-cea1-4487-b5c2-7459037109f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_devices = tf.config.list_physical_devices('MPS')\n",
    "if mps_devices:\n",
    "    print(\"Using MPS device\")\n",
    "    device_name = '/device:MPS:0'\n",
    "else:\n",
    "    print(\"MPS device not found, using CPU/GPU\")\n",
    "    device_name = '/device:CPU:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73eeb2-54e7-40ba-af7a-9ceac51c553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device_name):\n",
    "    rnn_model = Sequential([\n",
    "        SimpleRNN(50, activation='tanh', input_shape=(sequence_length, num_features)),\n",
    "        Dense(1)  # Regression output for totalFare\n",
    "    ])\n",
    "    rnn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"RNN model summary:\")\n",
    "    rnn_model.summary()\n",
    "\n",
    "    history_rnn = rnn_model.fit(\n",
    "        X_train_seq_scaled, y_train_seq_scaled,\n",
    "        epochs=50, batch_size=32,\n",
    "        validation_data=(X_test_seq_scaled, y_test_seq_scaled)\n",
    "    )\n",
    "\n",
    "    # Predict and reverse the scaling for evaluation\n",
    "    y_pred_rnn = rnn_model.predict(X_test_seq_scaled)\n",
    "    y_pred_rnn_original = scaler_y.inverse_transform(y_pred_rnn)\n",
    "    y_test_rnn_original = scaler_y.inverse_transform(y_test_seq_scaled)\n",
    "\n",
    "    mae_rnn = mean_absolute_error(y_test_rnn_original, y_pred_rnn_original)\n",
    "    rmse_rnn = np.sqrt(mean_squared_error(y_test_rnn_original, y_pred_rnn_original))\n",
    "    r2_rnn = r2_score(y_test_rnn_original, y_pred_rnn_original)\n",
    "    print(f\"RNN Model - MAE: {mae_rnn:.2f}, RMSE: {rmse_rnn:.2f}, R²: {r2_rnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2eb5a-f314-4718-8493-d456baae81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device_name):\n",
    "    lstm_model = Sequential([\n",
    "        LSTM(50, activation='tanh', input_shape=(sequence_length, num_features)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"LSTM model summary:\")\n",
    "    lstm_model.summary()\n",
    "\n",
    "    history_lstm = lstm_model.fit(\n",
    "        X_train_seq_scaled, y_train_seq_scaled,\n",
    "        epochs=50, batch_size=32,\n",
    "        validation_data=(X_test_seq_scaled, y_test_seq_scaled)\n",
    "    )\n",
    "\n",
    "    # Predict and reverse the scaling for evaluation\n",
    "    y_pred_lstm = lstm_model.predict(X_test_seq_scaled)\n",
    "    y_pred_lstm_original = scaler_y.inverse_transform(y_pred_lstm)\n",
    "    y_test_lstm_original = scaler_y.inverse_transform(y_test_seq_scaled)\n",
    "\n",
    "    mae_lstm = mean_absolute_error(y_test_lstm_original, y_pred_lstm_original)\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm_original, y_pred_lstm_original))\n",
    "    r2_lstm = r2_score(y_test_lstm_original, y_pred_lstm_original)\n",
    "    print(f\"LSTM Model - MAE: {mae_lstm:.2f}, RMSE: {rmse_lstm:.2f}, R²: {r2_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5a333-4e27-42b9-b40d-1fc815356b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ffcae-dd5c-4b3c-8ff3-4f37f2e0c465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
